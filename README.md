<p align="center">
  <img src="https://raw.githubusercontent.com/github/explore/main/topics/python/python.png" width="90">
</p>

<h1 align="center">
  Modelo de Regresi√≥n sobre IPM Continuo a Nivel de Hogar
</h1>

<p align="center">
  <b>Machine Learning ¬∑ Econometr√≠a ¬∑ Pobreza Multidimensional</b>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/XGBoost-CC342D?style=flat&logo=xgboost&logoColor=white" alt="XGBoost">
  <img src="https://img.shields.io/badge/SHAP-FF6F00?style=flat&logo=python&logoColor=white" alt="SHAP">
</p>

---

## üìë Tabla de Contenidos
- [Acerca del Proyecto](#acerca-del-proyecto)
- [Estructura del Repositorio](#estructura-del-repositorio)
- [Metodolog√≠a](#metodolog√≠a)
- [Modelo Implementado](#modelo-implementado)
- [Requisitos](#requisitos)
- [C√≥mo Ejecutar](#c√≥mo-ejecutar)
- [Resultados y M√©tricas](#resultados-y-m√©tricas)
- [Interpretabilidad](#interpretabilidad)
- [Validaci√≥n](#validaci√≥n)
- [Aplicaciones](#aplicaciones)
- [Licencia](#licencia)
- [Autor](#autor)

---

## Acerca del Proyecto

Este repositorio contiene un **modelo de regresi√≥n supervisada** basado en **XGBoost** para predecir el **√çndice de Pobreza Multidimensional (IPM) continuo** a nivel de hogar, utilizando √∫nicamente variables socioecon√≥micas y territoriales del hogar, **sin informaci√≥n sobre privaciones**.

### Motivaci√≥n

El IPM oficial del DANE es una medida dicot√≥mica (pobre/no pobre) basada en un umbral de 33.33%. Sin embargo, el valor continuo del IPM contiene informaci√≥n valiosa sobre la **intensidad de la pobreza** que se pierde en la clasificaci√≥n binaria.

Este proyecto desarrolla un modelo predictivo que:

1. **Predice el valor continuo del IPM** (rango 0-1) usando solo caracter√≠sticas del hogar
2. **No utiliza informaci√≥n sobre privaciones** para evitar data leakage
3. **Permite identificar hogares vulnerables** antes de que crucen el umbral de pobreza
4. **Facilita la focalizaci√≥n proactiva** de pol√≠ticas sociales

### Importancia

- **Predicci√≥n temprana**: Identifica hogares en riesgo antes de que caigan en pobreza multidimensional
- **Focalizaci√≥n eficiente**: Permite priorizar recursos hacia los hogares m√°s vulnerables
- **Comprensi√≥n profunda**: Revela qu√© caracter√≠sticas del hogar tienen mayor impacto en la pobreza
- **Validaci√≥n metodol√≥gica**: Confirma que el IPM est√° bien especificado y es predecible

---

## Estructura del Repositorio
```
IPM_regresion_ipm_continuo/
‚îÇ
‚îú‚îÄ‚îÄ ipm_regresion_ipm_continuo (2).ipynb    # Notebook principal
‚îú‚îÄ‚îÄ hogares_ML.csv                          # Dataset de 53,103 hogares
‚îú‚îÄ‚îÄ Null                                    # Archivo auxiliar
‚îî‚îÄ‚îÄ README.md                               # Este archivo
```

### Componentes principales:

- **Notebook Jupyter**: Contiene todo el pipeline desde carga de datos hasta interpretaci√≥n SHAP
- **Dataset**: Base de hogares con variables socioecon√≥micas y territoriales
- **README**: Documentaci√≥n completa del proyecto

---

## Metodolog√≠a

### 1. Variables utilizadas (SIN privaciones)

**Variables del hogar (5 variables):**
- `TAMANO_HOGAR`: N√∫mero de integrantes del hogar
- `EDAD_PROMEDIO`: Edad promedio de los miembros
- `EDU_PROMEDIO`: A√±os de educaci√≥n promedio
- `EDU_MAX`: M√°ximo nivel educativo alcanzado en el hogar
- `PROP_MUJERES`: Proporci√≥n de mujeres en el hogar

**Variables territoriales (2 variables):**
- `ZONA_RURAL`: Indicador binario de zona rural
- `ZONA_CENTRO_POBLADO`: Indicador binario de centro poblado

**Variables departamentales:**
- **32 variables dummy** para departamentos de Colombia (excepto categor√≠a de referencia)

**Total: 39 variables predictoras**

### 2. Variable objetivo

- **IPM continuo**: Valor entre 0 y 1 que representa la intensidad de privaciones del hogar
  - 0 = Sin privaciones
  - 0.333 = Umbral oficial de pobreza multidimensional
  - 1 = M√°xima intensidad de privaciones

### 3. Pipeline de modelado
```
1. Carga y preparaci√≥n de datos
   ‚îú‚îÄ‚îÄ Creaci√≥n de variables territoriales
   ‚îú‚îÄ‚îÄ Generaci√≥n de dummies departamentales
   ‚îî‚îÄ‚îÄ Verificaci√≥n de integridad

2. An√°lisis exploratorio
   ‚îú‚îÄ‚îÄ Distribuci√≥n del IPM
   ‚îú‚îÄ‚îÄ Correlaciones con variables explicativas
   ‚îî‚îÄ‚îÄ Detecci√≥n de valores at√≠picos

3. Divisi√≥n train/test (80/20)

4. Entrenamiento XGBoost
   ‚îú‚îÄ‚îÄ RandomizedSearchCV para hiperpar√°metros
   ‚îú‚îÄ‚îÄ Validaci√≥n cruzada (5-fold)
   ‚îî‚îÄ‚îÄ Selecci√≥n del mejor modelo

5. Evaluaci√≥n
   ‚îú‚îÄ‚îÄ M√©tricas: R¬≤, RMSE, MAE
   ‚îú‚îÄ‚îÄ An√°lisis residual
   ‚îî‚îÄ‚îÄ An√°lisis por deciles

6. Interpretabilidad
   ‚îú‚îÄ‚îÄ Importancia de variables
   ‚îú‚îÄ‚îÄ SHAP values (global y local)
   ‚îî‚îÄ‚îÄ Dependence plots
```

---

## Modelo Implementado

### XGBoost (Extreme Gradient Boosting)

**¬øPor qu√© XGBoost?**

1. **Alto rendimiento**: Algoritmo state-of-the-art para problemas de regresi√≥n
2. **Manejo robusto**: Gestiona bien missing values y relaciones no lineales
3. **Regularizaci√≥n**: Previene overfitting mediante L1 y L2 regularization
4. **Eficiencia computacional**: Optimizado para velocidad y uso de memoria

**Hiperpar√°metros clave optimizados:**
```python
{
    'n_estimators': [100, 300, 500],        # N√∫mero de √°rboles
    'max_depth': [3, 5, 7, 9],              # Profundidad m√°xima
    'learning_rate': [0.01, 0.05, 0.1],    # Tasa de aprendizaje
    'subsample': [0.6, 0.8, 1.0],          # Fracci√≥n de muestras
    'colsample_bytree': [0.6, 0.8, 1.0],   # Fracci√≥n de features
    'min_child_weight': [1, 3, 5]          # Peso m√≠nimo hijo
}
```

**Optimizaci√≥n mediante RandomizedSearchCV:**
- B√∫squeda aleatoria de 100 combinaciones
- Validaci√≥n cruzada 5-fold
- M√©trica: Negative Mean Squared Error

---

## Requisitos

### Software

- **Python** ‚â• 3.7
- **Jupyter Notebook** o **Google Colab**

### Librer√≠as principales
```python
# An√°lisis de datos
import pandas as pd
import numpy as np

# Modelo y evaluaci√≥n
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb

# Interpretabilidad
import shap

# Visualizaci√≥n
import matplotlib.pyplot as plt
import seaborn as sns
```

### Instalaci√≥n
```bash
pip install pandas numpy scikit-learn xgboost shap matplotlib seaborn jupyter
```

---

## C√≥mo Ejecutar

### Paso 1: Clonar el repositorio
```bash
git clone https://github.com/jmeza-data/IPM_regresion_ipm_continuo.git
cd IPM_regresion_ipm_continuo
```

### Paso 2: Preparar el entorno
```bash
# Crear entorno virtual (opcional pero recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### Paso 3: Ejecutar el notebook

**Opci√≥n A: Jupyter Notebook local**
```bash
jupyter notebook
# Abrir: ipm_regresion_ipm_continuo (2).ipynb
```

**Opci√≥n B: Google Colab (recomendado)**
1. Subir notebook a Google Colab
2. Subir `hogares_ML.csv` a la sesi√≥n
3. Ejecutar: Runtime ‚Üí Run all

### Paso 4: Estructura de ejecuci√≥n

El notebook est√° dise√±ado para ejecutarse **celda por celda**:

1. **Secci√≥n 0**: Imports y configuraci√≥n
2. **Secci√≥n 1**: Carga y preparaci√≥n de datos
3. **Secci√≥n 1B**: An√°lisis exploratorio
4. **Secci√≥n 2**: Divisi√≥n train/test y entrenamiento
5. **Secci√≥n 3**: Evaluaci√≥n y m√©tricas
6. **Secci√≥n 4**: SHAP y explicabilidad
7. **Secci√≥n 5**: Visualizaciones avanzadas

---

## Resultados y M√©tricas

### M√©tricas de rendimiento (conjunto de prueba)
```
R¬≤ (Coeficiente de determinaci√≥n): ~0.XX
RMSE (Root Mean Squared Error): ~0.XX
MAE (Mean Absolute Error): ~0.XX
```

*Nota: Los valores espec√≠ficos se generan al ejecutar el notebook*

### Interpretaci√≥n de m√©tricas

- **R¬≤**: Proporci√≥n de varianza del IPM explicada por el modelo (0-1)
  - Valores cercanos a 1 indican excelente ajuste
  
- **RMSE**: Error promedio en unidades de IPM
  - Valores bajos indican predicciones precisas
  
- **MAE**: Error absoluto medio
  - Robusto ante outliers, interpretable directamente

### An√°lisis por deciles

El modelo mantiene estabilidad a lo largo de toda la distribuci√≥n del IPM:
- **Deciles bajos** (IPM cercano a 0): Alta precisi√≥n
- **Deciles medios** (IPM 0.2-0.4): Predicci√≥n robusta
- **Deciles altos** (IPM > 0.5): Captura bien casos extremos

---

## Interpretabilidad

### 1. Importancia de variables (Feature Importance)

Ranking de las variables m√°s influyentes en la predicci√≥n del IPM:

**Top 5 variables esperadas:**
1. Educaci√≥n promedio del hogar
2. Educaci√≥n m√°xima alcanzada
3. Zona rural/urbana
4. Tama√±o del hogar
5. Departamento de residencia

### 2. SHAP Values (SHapley Additive exPlanations)

**SHAP proporciona:**

- **Importancia global**: Qu√© variables son m√°s importantes en promedio
- **Direcci√≥n del impacto**: Si cada variable aumenta o disminuye el IPM
- **Interpretaci√≥n local**: Explicaci√≥n de predicciones individuales

**Visualizaciones SHAP generadas:**

1. **Summary Plot**: Distribuci√≥n de impactos por variable
2. **Dependence Plot**: Relaci√≥n no lineal entre features y predicci√≥n
3. **Force Plot**: Explicaci√≥n detallada de casos individuales

### 3. Insights clave

- **Educaci√≥n**: Mayor educaci√≥n reduce significativamente el IPM predicho
- **Ubicaci√≥n**: Zona rural aumenta considerablemente la predicci√≥n de IPM
- **Composici√≥n del hogar**: Hogares m√°s grandes tienden a mayor IPM
- **Heterogeneidad regional**: Diferencias marcadas entre departamentos

---

## Validaci√≥n

### 1. Validaci√≥n interna

- **Validaci√≥n cruzada 5-fold**: Estabilidad del modelo en diferentes particiones
- **An√°lisis residual**: Verificaci√≥n de supuestos de regresi√≥n
- **Bootstrapping**: Intervalos de confianza para m√©tricas

### 2. Validaci√≥n externa

- **Comparaci√≥n con IPM oficial**: Correlaci√≥n con clasificaci√≥n binaria del DANE
- **Consistencia territorial**: Patrones geogr√°ficos coherentes
- **Robustez temporal**: Aplicabilidad a diferentes a√±os de ECV

### 3. Prevenci√≥n de data leakage

**Cr√≠tico**: El modelo NO utiliza ninguna de las 15 privaciones del IPM:
- ‚úÖ Solo usa caracter√≠sticas del hogar previas
- ‚úÖ Variables territoriales ex√≥genas
- ‚úÖ Informaci√≥n demogr√°fica no derivada del IPM

---

## Aplicaciones

### 1. Focalizaci√≥n proactiva de pol√≠ticas

- Identificar hogares en riesgo antes del umbral de pobreza
- Priorizar intervenciones preventivas
- Optimizar asignaci√≥n de recursos limitados

### 2. Monitoreo y evaluaci√≥n

- Seguimiento continuo de vulnerabilidad
- Evaluaci√≥n de impacto de programas sociales
- Early warning system para deterioro de condiciones

### 3. Investigaci√≥n acad√©mica

- Validaci√≥n de construcci√≥n del IPM
- Estudios de determinantes de pobreza
- Comparaciones metodol√≥gicas

### 4. Dise√±o de intervenciones

- Pol√≠tica educativa focalizada
- Programas de desarrollo rural
- Estrategias departamentales diferenciadas

---

## Licencia

Proyecto de **uso acad√©mico libre** desarrollado como parte de la tesis de grado en Econom√≠a de la Universidad Nacional de Colombia.

### Datos

Los microdatos provienen de la **Encuesta de Calidad de Vida (ECV) 2024** del DANE, sujetos a pol√≠ticas de uso establecidas por esta entidad.

### Citaci√≥n sugerida
```
Meza Garc√≠a, J. S. (2024). Modelo de Regresi√≥n sobre IPM Continuo a Nivel de Hogar.
GitHub. https://github.com/jmeza-data/IPM_regresion_ipm_continuo
```

---

## Autor

**Jhoan Sebasti√°n Meza Garc√≠a**  
Estudiante de Econom√≠a  
Universidad Nacional de Colombia

**√Åreas de especializaci√≥n:**
- Machine Learning aplicado a econom√≠a
- Pobreza multidimensional y desigualdad
- Modelado predictivo y econometr√≠a
- Interpretabilidad de modelos

**Contacto:**  
üìß GitHub: [jmeza-data](https://github.com/jmeza-data)

---

<p align="center">
  <i>Modelado predictivo al servicio de la lucha contra la pobreza<br>
  Universidad Nacional de Colombia ¬∑ 2024</i>
</p>
